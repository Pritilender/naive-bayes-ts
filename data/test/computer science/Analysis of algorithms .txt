computer science	analysis algorithms navigation search this article includes list references but its sources remain unclear because has insufficient inline citations please help improve this article introducing more precise march computer science the analysis algorithms the determination the amount resources such time and necessary execute them most algorithms are designed work with inputs arbitrary usually the efficiency running time algorithm stated function relating the input length the number steps time complexity storage locations space complexity algorithm analysis important part broader computational complexity theory which provides theoretical estimates for the resources needed any algorithm which solves given computational problem these estimates provide insight into reasonable directions search for efficient algorithms theoretical analysis algorithms common estimate their complexity the asymptotic estimate the complexity function for arbitrarily large big notation big omega notation and big theta notation are used this for binary search said run number steps proportional the logarithm the length the list being colloquially logarithmic time usually asymptotic estimates are used because different implementations the same algorithm may differ however the efficiencies any two reasonable implementations given algorithm are related constant multiplicative factor called hidden constant exact not measures efficiency can sometimes computed but they usually require certain assumptions concerning the particular implementation the called model computation model computation may defined terms abstract computer turing machine and postulating that certain operations are executed unit for the sorted list which apply binary search has elements and can guarantee that each lookup element the list can done unit then most log time units are needed return contents cost models run time analysis shortcomings empirical metrics orders growth empirical orders growth evaluating time complexity growth rate analysis other resources relevance see also notes references cost models time efficiency estimates depend what define for the analysis correspond usefully the actual execution the time required perform step must guaranteed bounded above one must careful for some analyses count addition two numbers one this assumption may not warranted certain for the numbers involved computation may arbitrarily the time required single addition can longer assumed two cost models are generally the uniform cost model also called uniform cost measurement and similar assigns constant cost every machine regardless the size the numbers involved the logarithmic cost model also called logarithmic cost measurement and variations assigns cost every machine operation proportional the number bits involved the latter more cumbersome only employed when for example the analysis arbitrary precision arithmetic algorithms like those used cryptography key point which often overlooked that published lower bounds for problems are often given for model computation that more restricted than the set operations that you could use practice and therefore there are algorithms that are faster than what would naively thought run time analysis run time analysis theoretical classification that estimates and anticipates the increase running time algorithm its input size usually denoted increases run time efficiency topic great interest computer science program can take hours even years finish depending which algorithm implements see also performance analysis which the analysis time shortcomings empirical metrics since algorithms are platform independent given algorithm can implemented arbitrary programming language arbitrary computer running arbitrary operating system there are significant drawbacks using empirical approach gauge the comparative performance given set take example program that looks specific entry sorted list size suppose this program were implemented computer art using linear search algorithm and computer much slower using binary search algorithm benchmark testing the two computers running their respective programs might look something like the nanoseconds nanoseconds based these would easy jump the conclusion that computer running algorithm that far superior efficiency that computer however the size the list increased sufficient that conclusion dramatically demonstrated nanoseconds nanoseconds computer running the linear search exhibits linear growth the time directly proportional its input doubling the input size doubles the run quadrupling the input size quadruples the and the other computer running the binary search exhibits logarithmic growth doubling the input size only increases the run time constant amount this even though computer ostensibly faster computer will inevitably surpass computer time because running algorithm with much slower growth orders growth big notation algorithm can said exhibit growth rate the order mathematical function beyond certain input size the function times positive constant provides upper bound limit for the time that other for given input size greater than some and constant the running time that algorithm will never larger than this concept frequently expressed using big for since the time insertion sort grows quadratically its input size insertion sort can said order big notation convenient way express the worst case scenario for given although can also used express the case for the case scenario for quicksort but the case time log empirical orders growth assuming the execution time follows power the coefficient can found taking empirical measurements run time some size points and calculating that the order growth indeed follows the power the empirical value will stay constant different and will change but still could serve for comparison any two given algorithms their empirical local orders growth behaviour applied the above nanoseconds nanoseconds clearly seen that the first algorithm exhibits linear order growth indeed following the power the empirical values for the second one are diminishing suggesting follows another rule growth and any case has much lower local orders growth and improving further empirically than the first evaluating time complexity the time complexity for the case scenario given algorithm can sometimes evaluated examining the structure the algorithm and making some simplifying consider the following pseudocode get positive integer from input print for for print print given computer will take discrete amount time execute each the instructions involved with carrying out this the specific amount time carry out given instruction will vary depending which instruction being executed and which computer executing but conventional this amount will deterministic say that the actions carried out step are considered consume time step uses time and the algorithm steps and will only run for case should assumed that step will run thus the total amount time run steps and step the loops steps and are trickier the outer loop test step will execute times note that extra step required terminate the for hence and not which will consume time the inner the other governed the value which iterates from the first pass through the outer iterates from the inner loop makes one running the inner loop body step consumes time and the inner loop test step consumes time during the next pass through the outer iterates from the inner loop makes two running the inner loop body step consumes time and the inner loop test step consumes time altogether the total time required run the inner loop body can expressed arithmetic progression which can factored the total time required run the outer loop test can evaluated which can factored therefore the total running time for this algorithm which reduces rule thumb one can assume that the order term any given function dominates its rate growth and thus defines its time this the order one can conclude that formally this can proven prove that for let constant greater than equal for more elegant approach analyzing this algorithm would declare that are all equal one unit system units chosen that one unit greater than equal the actual times for these this would mean that the running time breaks down for growth rate analysis other resources the methodology time analysis can also utilized for predicting other growth such consumption memory space consider the following pseudocode which manages and reallocates memory usage program based the size file which that program while file still open let size file for every kilobytes increase file size double the amount memory reserved this the file size memory will consumed exponential growth rate which order this extremely rapid and most likely unmanageable growth rate for consumption memory resources relevance algorithm analysis important practice because the accidental unintentional use inefficient algorithm can significantly impact system sensitive algorithm taking too long run can render its results outdated inefficient algorithm can also end requiring uneconomical amount computing power storage order again rendering practically see also amortized analysis asymptotic computational complexity worst and average case big notation computational complexity theory master theorem complete numerical analysis polynomial time program optimization profiling computer scalability smoothed analysis time complexity includes table orders growth for common algorithms 