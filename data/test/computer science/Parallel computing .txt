computer science
parallel computing navigation search blue gene massively parallel supercomputer parallel computing form computation which many calculations are carried out operating the principle that large problems can often divided into smaller which are then solved concurrently there are several different forms parallel bit level instruction level data and task parallelism parallelism has been employed for many mainly high performance computing but interest has grown lately due the physical constraints preventing frequency scaling power consumption and consequently heat computers has become concern recent parallel computing has become the dominant paradigm computer architecture mainly the form multi core processors parallel computers can roughly classified according the level which the hardware supports with multi core and multi processor computers having multiple processing elements within single while clusters mpps and grids use multiple computers work the same specialized parallel computer architectures are sometimes used alongside traditional for accelerating specific parallel computer programs are more difficult write than sequential because concurrency introduces several new classes potential software bugs which race conditions are the most communication and synchronization between the different subtasks are typically some the greatest obstacles getting good parallel program the maximum possible speed single program result parallelization known amdahl law contents background amdahl law and law dependencies race mutual synchronization and parallel slowdown fine coarse and embarrassing parallelism consistency models flynn taxonomy types parallelism bit level parallelism instruction level parallelism task parallelism hardware memory and communication classes parallel computers multicore computing symmetric multiprocessing distributed computing cluster computing massive parallel processing grid computing specialized parallel computers reconfigurable computing with programmable gate arrays general purpose computing graphics processing units application specific integrated circuits vector processors software parallel programming languages automatic parallelization application checkpointing algorithmic methods fault tolerance history see also references further reading external links background traditionally computer software has been written for serial solve algorithm constructed and implemented serial stream these instructions are executed central processing unit one only one instruction may execute after that instruction the next parallel the other uses multiple processing elements simultaneously solve this accomplished breaking the problem into independent parts that each processing element can execute its part the algorithm simultaneously with the the processing elements can diverse and include resources such single computer with multiple several networked specialized any combination the frequency scaling was the dominant reason for improvements computer performance from the until the runtime program equal the number instructions multiplied the average time per maintaining everything else increasing the clock frequency decreases the average time takes execute increase frequency thus decreases runtime for all compute bound programs however power consumption chip given the equation where the capacitance being switched per clock cycle proportional the number transistors whose inputs voltage and the processor frequency cycles per increases frequency increase the amount power used increasing processor power consumption led ultimately intel may cancellation its tejas and jayhawk processors which generally cited the end frequency scaling the dominant computer architecture moore law the empirical observation that transistor density microprocessor doubles every despite power consumption and repeated predictions its moore law still with the end frequency these additional transistors which are longer used for frequency can used add extra hardware for parallel amdahl law and law amdahl law the from parallelization would doubling the number processing elements should halve the and doubling second time should again halve the very few parallel algorithms achieve optimal most them have linear for small numbers processing which flattens out into constant value for large numbers processing the potential algorithm parallel computing platform given amdahl law originally formulated gene amdahl the states that small portion the program which cannot parallelized will limit the overall available from program solving large mathematical engineering problem will typically consist several parallelizable parts and several parallelizable parts the fraction running time program spends parallelizable then the maximum with parallelization the the sequential portion program accounts for the runtime can get more than speed regardless how many processors are this puts upper limit the usefulness adding more parallel execution when task cannot partitioned because sequential the application more effort has effect the the bearing child takes nine matter how many women are gustafson law another law closely related states that the speedup with processors programmer both law and law assume that the running time the sequential portion the program independent the number amdahl law assumes that the entire problem fixed size that the total amount work done parallel also independent the number processors whereas law assumes that the total amount work done parallel varies linearly with the number processors dependencies understanding data dependencies fundamental implementing parallel algorithms program can run more quickly than the longest chain dependent calculations known the critical path since calculations that depend upon prior calculations the chain must executed most algorithms not consist just long chain dependent there are usually opportunities execute independent calculations let and two program conditions describe when the two are independent and can executed for let all the input variables and the output and likewise for and are independent they satisfy violation the first condition introduces flow corresponding the first segment producing result used the second the second condition represents when the second segment produces variable needed the first segment the third and final condition represents output when two segments write the same the result comes from the logically last executed consider the following which demonstrate several kinds operation cannot executed before even parallel operation because uses result from violates and thus introduces flow this there are dependencies between the they can all run bernstein conditions not allow memory shared between different for some means enforcing ordering between accesses such semaphores barriers some other synchronization method race mutual synchronization and parallel slowdown subtasks parallel program are often called threads some parallel computer architectures use lightweight versions threads known fibers while others use bigger versions known processes threads generally accepted generic term for threads will often need update some variable that shared between the instructions between the two programs may interleaved any for consider the following instruction executed between and instruction executed between and the program will produce incorrect this known race condition the programmer must use lock provide mutual exclusion lock programming language construct that allows one thread take control variable and prevent other threads from reading writing until that variable the thread holding the lock free execute its critical section the section program that requires exclusive access some and unlock the data when therefore guarantee correct program the above program can rewritten use one thread will successfully lock variable while the other thread will locked out unable proceed until unlocked this guarantees correct execution the locks while necessary ensure correct program can greatly slow locking multiple variables using non atomic locks introduces the possibility program deadlock atomic lock locks multiple variables all cannot lock all does not lock any two threads each need lock the same two variables using atomic possible that one thread will lock one them and the second thread will lock the second such neither thread can and deadlock many parallel programs require that their subtasks act synchrony this requires the use barrier barriers are typically implemented using software one class known lock free and free algorithms altogether avoids the use locks and however this approach generally difficult implement and requires correctly designed data not all parallelization results generally task split into more and more those threads spend increasing portion their time communicating with each eventually the overhead from communication dominates the time spent solving the and further parallelization that splitting the workload over even more increases rather than decreases the amount time required this known parallel slowdown fine coarse and embarrassing parallelism applications are often classified according how often their subtasks need synchronize communicate with each application exhibits grained parallelism its subtasks must communicate many times per exhibits grained parallelism they not communicate many times per and embarrassingly parallel they rarely never have embarrassingly parallel applications are considered the easiest consistency models consistency model parallel programming languages and parallel computers must have consistency model also known memory the consistency model defines rules for how operations computer memory occur and how results are one the first consistency models was leslie lamport sequential consistency model sequential consistency the property parallel program that its parallel execution produces the same results sequential program sequentially consistent the results any execution the same the operations all the processors were executed some sequential and the operations each individual processor appear this sequence the order specified its software transactional memory common type consistency software transactional memory borrows from database theory the concept atomic transactions and applies them memory mathematically these models can represented several petri nets which were introduced carl adam doctoral were early attempt codify the rules consistency dataflow theory later built upon and dataflow architectures were created physically implement the ideas dataflow beginning the late process calculi such calculus communicating systems and communicating sequential processes were developed permit algebraic reasoning about systems composed interacting more recent additions the process calculus such the calculus have added the capability for reasoning about dynamic logics such tla and mathematical models such traces and actor event diagrams have also been developed describe the behavior concurrent flynn taxonomy michael flynn created one the earliest classification systems for parallel and computers and now known flynn taxonomy flynn classified programs and computers whether they were operating using single set multiple sets and whether not those instructions were using single set multiple sets flynn taxonomy sisd misd simd mimd the data classification equivalent entirely sequential the data classification analogous doing the same operation repeatedly over large data this commonly done signal processing applications multiple data rarely used while computer architectures deal with this were devised such systolic arrays few applications that fit this class multiple data programs are far the most common type parallel according david patterson and john hennessy some machines are hybrids these but this classic model has survived because easy and gives good first perhaps because its the most widely used types parallelism bit level parallelism bit level parallelism from the advent very scale integration computer chip fabrication technology the until about speed computer architecture was driven doubling computer word size the amount information the processor can manipulate per increasing the word size reduces the number instructions the processor must execute perform operation variables whose sizes are greater than the length the for where bit processor must add two bit integers the processor must first add the order bits from each integer using the standard addition then add the order bits using carry instruction and the carry bit from the lower order thus bit processor requires two instructions complete single where bit processor would able complete the operation with single historically bit microprocessors were replaced with then then bit this trend generally came end with the introduction bit which has been standard purpose computing for two not until recently with the advent architectures have bit processors become instruction level parallelism instruction level parallelism risc computer stream instructions executed these instructions can ordered and combined into groups which are then executed parallel without changing the result the this known level advances level parallelism dominated computer architecture from the until the modern processors have stage instruction pipelines each stage the pipeline corresponds different action the processor performs that instruction that processor with stage pipeline can have different instructions different stages the canonical example pipelined processor risc processor with five instruction decode execute memory and write the pentium processor had stage superscalar processor addition level parallelism from some processors can issue more than one instruction these are known superscalar processors instructions can grouped together only there data dependency between scoreboarding and the tomasulo algorithm which similar scoreboarding but makes use register renaming are two the most common techniques for implementing order execution and level task parallelism task parallelism task parallelism the characteristic parallel program that entirely different calculations can performed either the same different sets this contrasts with data where the same calculation performed the same different sets task parallelism does not usually scale with the size hardware memory and communication main memory parallel computer either shared memory shared between all processing elements single address space distributed memory which each processing element has its own local address distributed memory refers the fact that the memory logically but often implies that physically distributed distributed shared memory and memory virtualization combine the two where the processing element has its own local memory and access the memory local accesses local memory are typically faster than accesses local non uniform memory access computer architectures which each element main memory can accessed with equal latency and bandwidth are known uniform memory access systems typically that can achieved only shared memory system which the memory not physically system that does not have this property known non uniform memory access architecture distributed memory systems have uniform memory computer systems make use caches fast memories located close the processor which store temporary copies memory values nearby both the physical and logical parallel computer systems have difficulties with caches that may store the same value more than one with the possibility incorrect program these computers require cache coherency system which keeps track cached values and strategically purges thus ensuring correct program bus snooping one the most common methods for keeping track which values are being accessed and thus should designing high performance cache coherence systems very difficult problem computer shared memory computer architectures not scale well distributed memory systems processor processor and memory communication can implemented hardware several including via shared either multiported multiplexed memory crossbar switch shared bus interconnect network myriad topologies including star ring tree hypercube fat hypercube hypercube with more than one processor dimensional mesh parallel computers based interconnect networks need have some kind routing enable the passing messages between nodes that are not directly the medium used for communication between the processors likely hierarchical large multiprocessor classes parallel computers parallel computers can roughly classified according the level which the hardware supports this classification broadly analogous the distance between basic computing these are not mutually for clusters symmetric multiprocessors are relatively multicore computing multi core multicore processor processor that includes multiple execution units cores the same these processors differ from superscalar processors which can issue multiple instructions per cycle from one instruction stream multicore processor can issue multiple instructions per cycle from multiple instruction ibm cell microprocessor designed for use the sony playstation another prominent multicore each core multicore processor can potentially superscalar that every each core can issue multiple instructions from one instruction simultaneous multithreading which hyperthreading the best was early form processor capable simultaneous multithreading has only one execution unit core but when that execution unit idling such during cache miss uses that execution unit process second symmetric multiprocessing symmetric multiprocessing symmetric multiprocessor computer system with multiple identical processors that share memory and connect via bus contention prevents bus architectures from smps generally not comprise more than because the small size the processors and the significant reduction the requirements for bus bandwidth achieved large such symmetric multiprocessors are extremely provided that sufficient amount memory bandwidth distributed computing distributed computing distributed computer also known distributed memory distributed memory computer system which the processing elements are connected distributed computers are highly cluster computing computer cluster beowulf cluster cluster group loosely coupled computers that work together that some respects they can regarded single clusters are composed multiple standalone machines connected while machines cluster not have load balancing more difficult they are the most common type cluster the beowulf cluster which cluster implemented multiple identical commercial shelf computers connected with tcp ethernet local area network beowulf technology was originally developed thomas sterling and donald becker the vast majority the top supercomputers are massive parallel processing massively parallel massively parallel processor single computer with many networked mpps have many the same characteristics but mpps have specialized interconnect networks whereas clusters use commodity hardware for mpps also tend larger than typically having far than each cpu contains its own memory and copy the operating system and each subsystem communicates with the others via speed blue gene blue the fifth fastest supercomputer the world according the june top ranking grid computing distributed computing distributed computing the most distributed form parallel makes use computers communicating over the internet work given because the low bandwidth and extremely high latency available the distributed computing typically deals only with embarrassingly parallel problems many distributed computing applications have been which seti home and folding home are the known most grid computing applications use middleware software that sits between the operating system and the application manage network resources and standardize the software the most common distributed computing middleware the berkeley open infrastructure for network computing often distributed computing software makes use spare performing computations times when computer specialized parallel computers within parallel there are specialized parallel devices that remain niche areas while not domain specific they tend applicable only few classes parallel reconfigurable computing with programmable gate arrays reconfigurable computing the use field programmable gate array processor purpose fpga computer chip that can rewire itself for given fpgas can programmed with hardware description languages such vhdl verilog however programming these languages can several vendors have created hdl languages that attempt emulate the syntax and semantics the programming language with which most programmers are the best known hdl languages are mitrion impulse dime and handel specific subsets systemc based can also used for this amd decision open its hypertransport technology party vendors has become the enabling technology for performance reconfigurable according michael chief operating officer drc computer corporation when first walked into they called the socket stealers now they call their general purpose computing graphics processing units gpgpu tesla gpgpu card purpose computing graphics processing units fairly recent trend computer engineering gpus are processors that have been heavily optimized for computer graphics processing computer graphics processing field dominated data parallel particularly linear algebra matrix operations the early gpgpu programs used the normal graphics apis for executing however several new programming languages and platforms have been built general purpose computation gpus with both nvidia and amd releasing programming environments with cuda and stream sdk respectively other gpu programming languages include brookgpu peakstream and rapidmind nvidia has also released specific products for computation their tesla series the technology consortium khronos group has released the opencl specification which framework for writing programs that execute across platforms consisting cpus and amd apple intel nvidia and others are supporting opencl application specific integrated circuits application specific integrated circuit several application specific integrated circuit approaches have been devised for dealing with parallel because asic specific given can fully optimized for that for given asic tends outperform purpose however asics are created ray lithography this process requires which can extremely single mask can cost over million the smaller the transistors required for the the more expensive the mask will meanwhile performance increases purpose computing over time described tend wipe out these gains only one two chip high initial and the tendency overtaken driven purpose has rendered asics unfeasible for most parallel computing however some have been one example the flop riken machine which uses custom asics for molecular dynamics simulation vector processors vector processor vector processor cpu computer system that can execute the same instruction large sets vector processors have level operations that work linear arrays numbers example vector operation where and are each element vectors bit floating point numbers they are closely related simd cray computers became famous for their processing computers the and however vector both cpus and full computer have generally modern processor instruction sets include some vector processing such with altivec and streaming simd extensions software parallel programming languages list concurrent and parallel programming languages concurrent programming languages libraries apis and parallel programming models such algorithmic skeletons have been created for programming parallel these can generally divided into classes based the assumptions they make about the underlying memory shared distributed shared distributed shared memory programming languages communicate manipulating shared memory distributed memory uses message passing posix threads and openmp are two most widely used shared memory whereas message passing interface the most widely used passing system one concept used programming parallel programs the future concept where one part program promises deliver required datum another part program some future caps entreprise and pathscale are also coordinating their effort make hmpp hybrid multicore parallel directives open standard called openhmpp the openhmpp directive based programming model offers syntax efficiently offload computations hardware accelerators and optimize data movement from the hardware openhmpp directives describe remote procedure call accelerator device gpu more generally set the directives annotate fortran codes describe two sets the offloading procedures denoted onto remote device and the optimization data transfers between the cpu main memory and the accelerator automatic parallelization automatic parallelization automatic parallelization sequential program compiler the holy grail parallel despite decades work compiler automatic parallelization has had only limited mainstream parallel programming languages remain either explicitly parallel partially implicit which programmer gives the compiler directives for few fully implicit parallel programming languages sisal parallel haskell system for fpgas mitrion vhdl and verilog application checkpointing application checkpointing computer system grows the mean time between failures usually application checkpointing technique whereby the computer system takes snapshot the record all current resource allocations and variable akin core dump this information can used restore the program the computer should application checkpointing means that the program has restart from only its last checkpoint rather than the while checkpointing provides benefits variety especially useful highly parallel systems with large number processors used high performance computing algorithmic methods parallel computers become larger and becomes feasible solve problems that previously took too long parallel computing used wide range from bioinformatics protein folding and sequence analysis economics mathematical finance common types problems found parallel computing applications dense linear algebra sparse linear algebra spectral methods such cooley tukey fast fourier transform body problems such barnes hut simulation structured grid problems such lattice boltzmann methods unstructured grid problems such found finite element analysis monte carlo simulation combinational logic such brute force cryptographic techniques graph traversal such sorting algorithms dynamic programming branch and bound methods graphical models such detecting hidden markov models and constructing bayesian networks finite state machine simulation tolerance fault tolerant computer system this section requires expansion march parallel computing can also applied the design fault tolerant computer systems particularly via lockstep systems performing the same operation this provides redundancy case one component should and also allows automatic error detection and error correction the results history history computing illiac the origins true parallelism back federico conte menabrea and his sketch the analytic engine invented charles babbage ibm introduced the through project which gene amdahl was one the principal became the first commercially available computer use fully automatic floating point arithmetic april gill discussed parallel programming and the need for branching and also ibm researchers john cocke and daniel slotnick discussed the use parallelism numerical calculations for the first burroughs corporation introduced the processor computer that accessed memory modules through crossbar switch amdahl and slotnick published debate about the feasibility parallel processing american federation information processing societies was during this debate that law was coined define the limit due company honeywell introduced its first multics symmetric multiprocessor system capable running eight processors mmp processor project carnegie mellon university was among the first multiprocessors with more than few the first connected processor with snooping caches was the synapse simd parallel computers can traced back the the motivation behind early simd computers was amortize the gate delay the control unit over multiple slotnick had proposed building massively parallel computer for the lawrence livermore national laboratory his design was funded the air force which was the earliest simd computing illiac the key its design was fairly high with which allowed the machine work large datasets what would later known vector processing however illiac was called the most infamous because the project was only one fourth but took years and cost almost four times the original when was finally ready run its first real application was outperformed existing commercial supercomputers such the cray see also list important publications parallel and distributed computing list distributed computing conferences concurrency computer synchronous programming content addressable parallel processor transputer 