computer science
data compression navigation search source code computer science and information theory data compression source coding bit rate reduction involves encoding information using fewer bits than the original compression can either lossy lossless lossless compression reduces bits identifying and eliminating statistical redundancy information lost lossless lossy compression reduces bits identifying unnecessary information and removing the process reducing the size data file popularly referred data although its formal name source coding coding done the source the data before stored compression useful because helps reduce resource such data storage space transmission capacity because compressed data must decompressed this extra processing imposes computational other costs through this situation far from being free lunch data compression subject space time complexity trade for compression scheme for video may require expensive hardware for the video decompressed fast enough viewed being and the option decompress the video full before watching may inconvenient require additional the design data compression schemes involves offs among various including the degree the amount distortion introduced when using lossy data compression and the computational resources required compress and uncompress the contents lossless lossy theory machine learning data differencing outlook and currently unused potential uses audio lossy audio compression coding methods speech encoding history video encoding theory timeline genetics see also references external links lossless lossless data compression algorithms usually exploit statistical redundancy represent data more concisely without losing information that the process lossless compression possible because most world data has statistical for image may have areas colour that not change over several instead coding red red the data may encoded red this basic example run length encoding there are many schemes reduce file size eliminating the lempel ziv compression methods are among the most popular algorithms for lossless deflate variation optimized for decompression speed and compression but compression can deflate used pkzip gzip and png lzw used gif images also noteworthy the lzr algorithm which serves the basis for the zip method methods use based compression model where table entries are substituted for repeated strings for most this table generated dynamically from earlier data the the table itself often huffman encoded shri lzx current based coding scheme that performs well lzx used cab format the best modern lossless compressors use probabilistic models such prediction partial matching the burrows wheeler transform can also viewed indirect form statistical the class grammar based codes are gaining popularity because they can compress highly repetitive extremely for biological data collection same related huge versioned document internet etc the basic task based codes constructing free grammar deriving single sequitur and pair are practical grammar compression algorithms for which public codes are further refinement these statistical predictions can coupled algorithm called arithmetic coding arithmetic invented jorma rissanen and turned into practical method neal and achieves superior compression the known huffman algorithm and lends itself especially well adaptive data compression tasks where the predictions are strongly arithmetic coding used the level image compression standard jbig and the document compression standard djvu the text entry system dasher inverse arithmetic lossy lossy data compression the converse lossless data compression these some loss information dropping nonessential detail from the data source can save storage lossy data compression schemes are informed research how people perceive the data for the human eye more sensitive subtle variations luminance than variations jpeg image compression works part rounding off nonessential bits there corresponding trade off between preserving information and reducing number popular compression formats exploit these perceptual including those used music files images and lossy image compression can used digital cameras increase storage capacities with minimal degradation picture similarly dvds use the lossy mpeg video codec for video compression lossy audio compression methods psychoacoustics are used remove audible less components the audio signal compression human speech often performed with even more specialized speech coding voice sometimes distinguished separate discipline from audio compression different audio and speech compression standards are listed under audio codecs voice compression used internet telephony for example audio compression used for ripping and decoded audio theory the theoretical background compression provided information theory which closely related algorithmic information theory for lossless compression and rate distortion theory for lossy these areas study were essentially forged claude shannon who published fundamental papers the topic the late and early coding theory also the idea data compression deeply connected with statistical inference machine learning machine learning there close connection between machine learning and system that predicts the posterior probabilities sequence given its entire history can used for optimal data compression using arithmetic coding the output while optimal compressor can used for prediction finding the symbol that compresses given the previous this equivalence has been used justification for data compression benchmark for general data differencing data differencing data compression can viewed special case data differencing data differencing consists producing difference given source and target with patching producing target given source and difference while data compression consists producing compressed file given and decompression consists producing target given only compressed thus one can consider data compression data differencing with empty source the compressed file corresponding difference from this the same considering absolute entropy corresponding data special case relative entropy corresponding data with initial when one wishes emphasize the one may use the term differential compression refer data outlook and currently unused potential estimated that the total amount the data that are stored the storage devices could further compressed with existing compression algorithms remaining average factor estimated that the combined technological capacity the world store information provides exabytes hardware digits but when the corresponding content optimally this only represents exabytes shannon information uses audio audio codec audio data distinguished from dynamic range compression has the potential reduce the transmission bandwidth and storage requirements audio audio compression algorithms are implemented software audio codecs lossy audio compression algorithms provide higher compression the cost fidelity and are used numerous audio these algorithms almost all rely psychoacoustics eliminate less audible meaningful thereby reducing the space required store transmit both lossy and lossless information redundancy using methods such coding pattern recognition and linear prediction reduce the amount information used represent the uncompressed the acceptable off between loss audio quality and transmission storage size depends upon the for one compact disc holds approximately one hour uncompressed high fidelity music less than hours music compressed hours music compressed the format medium bit rate digital sound recorder can typically store around hours clearly intelligible speech lossless audio compression produces representation digital data that decompress exact digital duplicate the original audio unlike playback from lossy compression techniques such vorbis and compression ratios are around original which similar those for generic lossless data lossless compression unable attain high compression ratios due the complexity waveforms and the rapid changes sound codecs like flac shorten and tta use linear prediction estimate the spectrum the many these algorithms use convolution with the filter slightly whiten flatten the thereby allowing traditional lossless compression work more the process reversed upon when audio files are either further compression for editing desirable work from unchanged original uncompressed losslessly processing lossily compressed file for some purpose usually produces final result inferior the creation the same compressed file from uncompressed addition sound editing lossless audio compression often used for archival master number lossless audio compression formats shorten was early lossless newer ones include free lossless audio codec apple apple lossless mpeg als microsoft windows media audio lossless wma monkey audio tta and wavpack see list lossless codecs for complete some audio formats feature combination lossy format and lossless this allows stripping the correction easily obtain lossy such formats include mpeg sls scalable wavpack and optimfrog dualstream other formats are associated with distinct such direct stream transfer used super audio meridian lossless packing used dvd audio dolby truehd blu ray and dvd lossy audio compression acoustic spectrograms lossy audio compression used wide range addition the direct applications players digitally compressed audio streams are used most video digital streaming media the internet satellite and cable and increasingly terrestrial radio lossy compression typically achieves far greater compression than lossless compression data percent percent the original rather than percent percent discarding critical the innovation lossy audio compression was use psychoacoustics recognize that not all data audio stream can perceived the human auditory system most lossy compression reduces perceptual redundancy first identifying perceptually irrelevant that sounds that are very hard typical examples include high frequencies sounds that occur the same time louder those sounds are coded with decreased accuracy not due the nature lossy audio quality suffers when file decompressed and recompressed digital generation loss this makes lossy compression unsuitable for storing the intermediate results professional audio engineering such sound editing and multitrack however they are very popular with end users particularly megabyte can store about worth music adequate coding methods determine what information audio signal perceptually most lossy compression algorithms use transforms such the modified discrete cosine transform convert time domain sampled waveforms into transform once typically into the frequency domain component frequencies can allocated bits according how audible they audibility spectral components calculated using the absolute threshold hearing and the principles simultaneous masking the phenomenon wherein signal masked another signal separated some temporal masking where signal masked another signal separated equal loudness contours may also used weight the perceptual importance models the human brain combination incorporating such effects are often called psychoacoustic models other types lossy such the linear predictive coding used with are based these coders use model the generator such the human vocal tract with whiten the audio signal flatten its before quantization lpc may thought basic perceptual coding reconstruction audio signal using linear predictor shapes the quantization noise into the spectrum the target partially masking lossy formats are often used for the distribution streaming audio interactive applications such the coding speech for digital transmission cell phone such the data must decompressed the data rather than after the entire data stream has been not all audio codecs can used for streaming and for such applications codec designed stream data effectively will usually latency results from the methods used encode and decode the some codecs will analyze longer segment the data optimize and then code manner that requires larger segment data one time often codecs create segments called frame create discrete data segments for encoding and the inherent latency the coding algorithm can for when there way transmission such with telephone significant delays may seriously degrade the perceived contrast the speed which proportional the number operations required the here latency refers the number samples that must analysed before block audio the minimum latency zero samples the decoder simply reduces the number bits used quantize the time domain algorithms such lpc also often have low hence their popularity speech coding for algorithms such however large number samples have analyzed implement psychoacoustic model the frequency and latency the order for way speech encoding speech encoding important category audio data the perceptual models used estimate what human ear can hear are generally somewhat different from those used for the range frequencies needed convey the sounds human voice are normally far narrower than that needed for and the sound normally less speech can encoded high quality using relatively low bit the data compressed analog such voltage that varies with quantization employed digitize into numbers normally this referred digital conversion the integers generated quantization are bits then the entire range the analog signal divided into intervals and all the signal values within interval are quantized the same bit integers are then the range the analog signal divided into intervals this relation illustrates the compromise between high resolution large number analog and high compression small integers this application quantization used several speech compression this some combination two only encoding sounds that could made single human throwing away more the data the keeping just enough reconstruct intelligible voice rather than the full frequency range human hearing perhaps the earliest algorithms used speech encoding and audio data compression were the law algorithm and the law algorithm history literature compendium for large variety audio coding systems was published the ieee journal selected areas communications february while there were some papers from before that this collection documented entire variety working audio nearly all them using perceptual masking techniques and some kind frequency analysis and end noiseless several these papers remarked the difficulty obtaining clean digital audio for research most not the authors the jsac edition were also active the audio the first commercial broadcast automation audio compression system was developed oscar engineering professor the university buenos aires using the psychoacoustic principle the masking critical bands first published started developing practical application based the recently developed ibm computer and the broadcast automation system was launched under the name audicom twenty years almost all the radio stations the world were using similar technology manufactured number video video compression format video compression uses modern coding techniques reduce redundancy video most video compression algorithms and codecs combine spatial image compression and temporal motion compensation video compression practical implementation source coding information most video codecs also use audio compression techniques parallel compress the but combined data streams one the majority video compression algorithms use lossy compression uncompressed video requires very high data rate although lossless video compression codecs perform average compression over factor typical mpeg lossy compression video has compression factor between and all lossy there trade off between video quality cost processing the compression and and system highly compressed video may present visible distracting artifacts some video compression schemes typically operates shaped groups neighboring pixels often called macroblocks these pixel groups blocks pixels are compared from one frame the and the video compression codec sends only the differences within those areas video with more the compression must encode more data keep with the larger number pixels that are commonly during flames flocks and some panning the frequency detail leads quality decreases increases the variable bitrate encoding theory video data may represented series still image the sequence frames contains spatial and temporal redundancy that video compression algorithms attempt eliminate code smaller similarities can encoded only storing differences between using perceptual features human for small differences color are more difficult perceive than are changes compression algorithms can average color across these similar areas reduce manner similar those used jpeg image some these methods are inherently lossy while others may preserve all relevant information from the uncompressed video one the most powerful techniques for compressing video interframe interframe compression uses one more earlier later frames sequence compress the current while intraframe compression uses only the current effectively being image compression the most powerful used method works comparing each frame the video with the previous the frame contains areas where nothing has the system simply issues short command that copies that part the previous bit into the next sections the frame move simple the compressor emits slightly command that tells the decompressor rotate lighten darken the this longer command still remains much shorter than intraframe interframe compression works well for programs that will simply played back the but can cause problems the video sequence needs because interframe compression copies data from one frame the original frame simply cut out lost the following frames cannot reconstructed some video formats such compress each frame independently using intraframe making compressed video almost easy editing uncompressed one finds the beginning and ending each and simply copies bit each frame that one wants and discards the frames one another difference between intraframe and interframe compression with intraframe each frame uses similar amount most interframe certain frames such frames mpeg aren allowed copy data from other they require much more data than other frames possible build based video editor that spots problems caused when frames are edited out while other frames need this has allowed newer formats like hdv used for however this process demands lot more computing power than editing intraframe compressed video with the same picture today nearly all commonly used video compression methods those standards approved the itu iso apply discrete cosine transform for spatial redundancy the dct that widely used this regard was introduced ahmed natarajan and rao other such fractal compression matching pursuit and the use discrete wavelet transform have been the subject some but are typically not used practical products except for the use wavelet coding image coders without motion interest fractal compression seems due recent theoretical analysis showing comparative lack effectiveness such timeline the following table partial history international video compression itu itu mpeg part iso iec part iso iec dvd video ray digital video broadcasting svcd itu mpeg part iso iec divx xvid avc sony panasonic samsung iso iec ray dvd digital video broadcasting ipod video apple smpte iso iec genetics compression genomic sequencing data genetics compression algorithms are the latest generation lossless algorithms that compress data typically sequences using both conventional compression algorithms and genetic algorithms adapted the specific team scientists from johns hopkins university published genetic compression algorithm that does not use reference genome for hapzipper was tailored for hapmap data and achieves over fold compression reduction file providing fold better compression and much faster time than the leading purpose compression for chanda elhaik and bader introduced maf based encoding which reduces the heterogeneity the dataset sorting snps their minor allele thus homogenizing the other algorithms and dnazip and have compression ratios allowing billion basepair diploid human genomes stored megabytes relative reference genome averaged over many see also this see also section may contain excessive number suggestions please ensure that only the most relevant suggestions are given and that they are not red links and consider integrating suggestions into the article november auditory masking calgary corpus canterbury corpus context mixing compression artifact data compression symmetry dictionary coder distributed source coding dyadic distribution dynamic markov compression elias gamma coding entropy encoding fibonacci coding golomb coding http compression kolmogorov complexity magic compression algorithm minimum description length code range encoding band coding universal code data vector quantization 