computer science	information theory navigation search information science information theory branch applied mathematics electrical engineering and computer science involving the quantification information information theory was developed claude shannon find fundamental limits signal processing operations such compressing data and reliably storing and communicating data since its inception has broadened find applications many other including statistical inference natural language processing cryptography neurobiology the evolution and function molecular model selection thermal quantum computing plagiarism detection and other forms data analysis key measure information entropy which usually expressed the average number bits needed store communicate one symbol entropy quantifies the uncertainty involved predicting the value random variable for specifying the outcome fair coin flip two equally likely provides less information lower than specifying the outcome from roll die six equally likely applications fundamental topics information theory include lossless data compression zip files lossy data compression and jpegs and channel coding for digital subscriber line the field the intersection mathematics statistics computer science physics neurobiology and electrical engineering its impact has been crucial the success the voyager missions deep the invention the compact the feasibility mobile the development the internet the study linguistics and human the understanding black holes and numerous other important fields information theory are source coding channel coding algorithmic complexity theory algorithmic information theory information theoretic security and measures contents overview historical background quantities information entropy joint entropy conditional entropy mutual information kullback leibler divergence information kullback leibler divergence prior from the truth other quantities coding theory source theory rate channel capacity capacity particular channel models applications other fields intelligence uses and secrecy applications pseudorandom number generation seismic exploration semiotics miscellaneous applications see also applications history theory concepts references the classic work other journal articles textbooks information theory other books external links overview the main concepts information theory can grasped considering the most widespread means human language two important aspects concise language are first the most common words the should shorter than less common words roundabout generation mediocre that sentences will not too such tradeoff word length analogous data compression and the essential aspect source coding second part sentence unheard misheard due noise passing car the listener should still able glean the meaning the underlying such robustness essential for electronic communication system for properly building such robustness into communications done channel coding source coding and channel coding are the fundamental concerns information note that these concerns have nothing with the importance for platitude such thank come takes about long say write the urgent call while the latter may more important and more meaningful many information however does not consider message importance these are matters the quality data rather than the quantity and readability the latter which determined solely information theory generally considered have been founded claude shannon his seminal mathematical theory communication the central paradigm classical information theory the engineering problem the transmission information over noisy the most fundamental results this theory are source coding theorem which establishes the number bits needed represent the result uncertain event given its entropy and noisy channel coding theorem which states that reliable communication possible over noisy channels provided that the rate communication below certain called the channel the channel capacity can approached practice using appropriate encoding and decoding information theory closely associated with collection pure and applied disciplines that have been investigated and reduced engineering practice under variety rubrics throughout the world over the past half century adaptive systems anticipatory systems artificial intelligence complex systems complexity science cybernetics informatics machine learning along with systems sciences many information theory broad and deep mathematical with equally broad and deep amongst which the vital field coding theory coding theory concerned with finding explicit called codes for increasing the efficiency and reducing the net error rate data communication over noisy channel near the limit that shannon proved the maximum possible for that these codes can roughly subdivided into data compression source and error correction channel techniques the latter took many years find the methods work proved were third class information theory codes are cryptographic algorithms both codes and ciphers concepts methods and results from coding theory and information theory are widely used cryptography and cryptanalysis see the article ban for historical information theory also used information retrieval intelligence gathering gambling statistics and even musical composition historical background history information theory the landmark event that established the discipline information and brought immediate worldwide was the publication claude shannon classic paper mathematical theory communication the bell system technical journal july and october prior this limited theoretic ideas had been developed bell labs all implicitly assuming events equal harry nyquist paper certain factors affecting telegraph speed contains theoretical section quantifying intelligence and the line which can transmitted communication giving the relation recalling boltzmann constant where the speed transmission the number different voltage levels choose from each time and ralph hartley paper transmission information uses the word information measurable reflecting the ability distinguish one sequence symbols from any thus quantifying information where was the number possible and the number symbols the natural unit information was therefore the decimal much later renamed the hartley his honour unit scale measure alan turing used similar ideas part the statistical analysis the breaking the german second world war enigma ciphers much the mathematics behind information theory with events different probabilities were developed for the field thermodynamics ludwig boltzmann and willard gibbs connections between theoretic entropy and thermodynamic including the important contributions rolf landauer the are explored entropy thermodynamics and information theory revolutionary and groundbreaking the work for which had been substantially completed bell labs the end shannon for the first time introduced the qualitative and quantitative model communication statistical process underlying information opening with the assertion that with came the ideas the information entropy and redundancy and its relevance through the source coding theorem the mutual information and the channel capacity noisy including the promise perfect free communication given the noisy channel coding theorem the practical result the shannon hartley law for the channel capacity gaussian channel well the bit new way seeing the most fundamental unit quantities information quantities information information theory based probability theory and statistics the most important quantities information are entropy the information random variable and mutual information the amount information common between two random the former quantity indicates how easily message data can compressed while the latter can used find the communication rate across channel the choice logarithmic base the following formulae determines the unit information entropy that the most common unit information the bit based the binary logarithm other units include the nat which based the natural logarithm and the hartley which based the common logarithm what expression the form considered convention equal zero whenever this justified because for any logarithmic entropy bernoulli trial binary entropy function the entropy discrete random variable measure the amount uncertainty associated with the value suppose one transmits bits and these bits are known ahead transmission certain value with absolute logic dictates that information has been however each equally and independently likely bits the information theoretic have been between these two information can quantified the set all messages that could and the probability some then the the self information which the entropy contribution individual and the expected value important property entropy that maximized when all the messages the message space are equiprobable most which case the special case information entropy for random variable with two outcomes the binary entropy function usually taken the logarithmic base joint entropy the joint entropy two discrete random variables and merely the entropy their this implies that and are independent then their joint entropy the sum their individual for represents the position chess piece the row and the then the joint entropy the row the piece and the column the piece will the entropy the position the despite similar joint entropy should not confused with cross entropy conditional entropy the conditional entropy conditional uncertainty given random variable also called the equivocation about the average conditional entropy over because entropy can conditioned random variable that random variable being certain care should taken not confuse these two definitions conditional the former which more common basic property this form conditional entropy mutual information mutual information measures the amount information that can obtained about one random variable observing important communication where can used maximize the amount information shared between sent and received the mutual information relative given where pecific mutual the pointwise mutual information basic property the mutual information that that knowing can save average bits encoding compared not knowing mutual information symmetric mutual information can expressed the average kullback leibler divergence information between the posterior probability distribution given the value and the prior distribution other this measure how the the probability distribution will change are given the value this often recalculated the divergence from the product the marginal distributions the actual joint mutual information closely related the log likelihood ratio test the context contingency tables and the multinomial distribution and pearson test mutual information can considered statistic for assessing independence between pair and has specified asymptotic kullback leibler divergence information the kullback leibler divergence information divergence information gain relative entropy way comparing two true probability distribution and arbitrary probability distribution compress data manner that assumes the distribution underlying some when the correct the leibler divergence the number average additional bits per datum necessary for thus defined although sometimes used distance divergence not true metric since not symmetric and does not satisfy the triangle inequality making kullback leibler divergence prior from the truth another interpretation divergence suppose number about drawn randomly from discrete set with probability distribution alice knows the true distribution while bob believes has that the distribution then bob will more surprised than upon seeing the value the divergence the expected value surprisal minus measured bits the log base this the extent which prior wrong can quantified terms how unnecessarily expected make other quantities other important information theoretic quantities include nyi entropy generalization differential entropy generalization quantities information continuous and the conditional mutual information coding theory coding theory error detection and correction coding theory one the most important and direct applications information can subdivided into source coding theory and channel coding theory using statistical description for information theory quantifies the number bits needed describe the which the information entropy the data compression source there are two formulations for the compression lossless data compression the data must reconstructed lossy data compression allocates bits needed reconstruct the within specified fidelity level measured distortion this subset information theory called rate distortion theory error correcting codes channel while data compression removes much redundancy error correcting code adds just the right kind redundancy error correction needed transmit the data efficiently and faithfully across noisy this division coding theory into compression and transmission justified the information transmission channel separation theorems that justify the use bits the universal currency for information many however these theorems only hold the situation where one transmitting user wishes communicate one receiving scenarios with more than one transmitter the access more than one receiver the broadcast channel intermediary helpers the relay channel more general networks compression followed transmission may longer network information theory refers these agent communication source theory any process that generates successive messages can considered source memoryless source one which each message independent identically distributed random variable whereas the properties ergodicity and stationarity impose more general all such sources are stochastic these terms are well studied their own right outside information rate information rate the average entropy per for memoryless this merely the entropy each while the case stationary stochastic that the conditional entropy symbol given all the previous symbols for the more general case process that not necessarily the average rate that the limit the joint entropy per for stationary these two expressions give the same common information theory speak the rate entropy this for when the source information english the rate source information related its redundancy and how well can compressed the subject source coding channel capacity channel capacity communications over such ethernet cable the primary motivation information anyone ever used telephone mobile knows however such channels often fail produce exact reconstruction noise periods and other forms signal corruption often degrade how much information can one hope communicate over noisy otherwise channel consider the communications process over discrete simple model the process shown here represents the space messages and the space messages received during unit time over our let the conditional probability distribution function given will consider inherent fixed property our communications channel representing the nature the noise our then the joint distribution and completely determined our channel and our choice the marginal distribution messages choose send over the under these would like maximize the rate the signal can communicate over the the appropriate measure for this the mutual information and this maximum mutual information called the channel capacity and given this capacity has the following property related communicating information rate where usually bits per for any information rate and coding error for large enough there exists code length and rate and decoding such that the maximal probability block error that always possible transmit with arbitrarily small block for any rate impossible transmit with arbitrarily small block channel coding concerned with finding such nearly optimal codes that can used transmit data over noisy channel with small coding error rate near the channel capacity particular channel models time analog communications channel subject gaussian noise see shannon hartley theorem binary symmetric channel with crossover probability binary binary output channel that flips the input bit with probability the bsc has capacity bits per channel where the binary entropy function the base logarithm binary erasure channel with erasure probability binary ternary output the possible channel outputs are and third symbol called the erasure represents complete loss information about input the capacity the bec bits per channel applications other fields intelligence uses and secrecy applications information theoretic concepts apply cryptography and cryptanalysis turing information the ban was used the ultra project breaking the german enigma machine code and hastening the end world war europe shannon himself defined important concept now called the unicity distance based the redundancy the plaintext attempts give minimum amount ciphertext necessary ensure unique information theory leads believe much more difficult keep secrets than might first brute force attack can break systems based asymmetric key algorithms most commonly used methods symmetric key algorithms sometimes called secret key such block ciphers the security all such methods currently comes from the assumption that known attack can break them practical amount information theoretic security refers methods such the one time pad that are not vulnerable such brute force such the positive conditional mutual information between the plaintext and ciphertext conditioned the key can ensure proper while the unconditional mutual information between the plaintext and ciphertext remains resulting absolutely secure other eavesdropper would not able improve his her guess the plaintext gaining knowledge the ciphertext but not the however any other cryptographic care must used correctly apply even theoretically secure the venona project was able crack the time pads the soviet union due their improper reuse key pseudorandom number generation pseudorandom number generators are widely available computer language libraries and application they almost unsuited cryptographic use they not evade the deterministic nature modern computer equipment and class improved random number generators termed cryptographically secure pseudorandom number generators but even they require external the software random seeds work these can obtained via extractors done the measure sufficient randomness extractors min entropy value related shannon entropy through nyi entropy nyi entropy also used evaluating randomness cryptographic although the distinctions among these measures mean that random variable with high shannon entropy not necessarily satisfactory for use extractor and for cryptography seismic exploration one early commercial application information theory was the field seismic oil work this field made possible strip off and separate the unwanted noise from the desired seismic information theory and digital signal processing offer major improvement resolution and image clarity over previous analog semiotics concepts from information theory such redundancy and code control have been used semioticians such umberto eco and landi explain ideology form message transmission whereby dominant social class emits its message using signs that exhibit high degree redundancy such that only one message decoded among selection competing miscellaneous applications information theory also has applications gambling and investing black holes bioinformatics and see also mathematics portal communication theory constructor theory generalization information theory that includes quantum information list important publications philosophy information applications cryptanalysis cryptography cybernetics entropy thermodynamics and information theory gambling intelligence information seismic exploration history hartley history information theory timeline information theory theory coding theory detection theory estimation theory fisher information information algebra information asymmetry information geometry information theory and measure theory kolmogorov complexity logic information network coding philosophy information quantum information science semiotic information theory source coding unsolved problems concepts ban channel capacity channel communication source conditional entropy covert channel decoder differential entropy encoder information entropy joint entropy leibler divergence mutual information pointwise mutual information receiver information redundancy nyi entropy information unicity distance variety 