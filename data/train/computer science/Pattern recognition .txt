computer science
pattern recognition navigation search pattern recognition machine learning data mining classification clustering regression anomaly detection association rules reinforcement learning structured prediction feature learning online learning supervised learning grammar induction supervised learning classification regression decision trees ensembles bagging boosting random forest linear regression naive bayes neural networks logistic regression perceptron support vector machine relevance vector machine clustering birch hierarchical means maximization dbscan optics shift dimensionality reduction factor analysis cca ica lda nmf pca sne structured prediction graphical models bayes net crf hmm anomaly detection local outlier factor neural nets autoencoder deep learning multilayer perceptron rnn restricted boltzmann machine som convolutional neural network variance dilemma computational learning theory empirical risk minimization pac learning statistical learning theory computer science portal statistics portal has been suggested that this article merged with machine learning discuss proposed since may this factual accuracy disputed please help ensure that disputed statements are reliably sourced see the relevant discussion the talk page may pattern recognition nearly synonymous with machine learning this branch artificial intelligence focuses the recognition patterns and regularities many these patterns are learned from labeled training data supervised learning but when labeled data are available other algorithms can used discover previously unknown patterns unsupervised learning the terms pattern machine data mining and knowledge discovery databases are hard they largely overlap their machine learning the common term for supervised learning methods and originates from artificial intelligence whereas kdd and data mining have larger focus unsupervised methods and stronger connection business pattern recognition has its origins engineering and the term popular the context computer vision leading computer vision conference named conference computer vision and pattern recognition pattern there may higher interest explain and visualize the whereas machine learning traditionally focuses maximizing the recognition yet all these domains have evolved substantially from their roots artificial engineering and and have become increasingly similar integrating developments and ideas from each machine learning pattern recognition the assignment label given input discriminant analysis was introduced for this same purpose example pattern recognition classification which attempts assign each input value one given set classes for determine whether given email spam however pattern recognition more general problem that encompasses other types output other examples are regression which assigns valued output each sequence labeling which assigns class each member sequence values for part speech tagging which assigns part speech each word input and parsing which assigns parse tree input describing the syntactic structure the pattern recognition algorithms generally aim provide reasonable answer for all possible inputs and perform most matching the taking into account their statistical this opposed pattern matching algorithms which look for exact matches the input with existing common example matching algorithm regular expression matching which looks for patterns given sort textual data and included the search capabilities many text editors and word processors contrast pattern pattern matching generally not considered type machine although matching algorithms especially with fairly carefully tailored can sometimes succeed providing quality output the sort provided recognition pattern recognition studied many including psychology psychiatry ethology cognitive science traffic flow and computer science contents overview probabilistic classifiers how many feature variables are problem statement supervised frequentist bayesian approach pattern uses algorithms classification algorithms supervised algorithms predicting categorical clustering algorithms unsupervised algorithms predicting categorical ensemble learning algorithms supervised algorithms for combining multiple learning algorithms general algorithms for predicting structured sets labels multilinear subspace learning algorithms predicting labels multidimensional data using tensor real valued sequence labeling algorithms predicting sequences valued regression algorithms predicting valued sequence labeling algorithms predicting sequences categorical see also references further reading external links overview pattern recognition generally categorized according the type learning procedure used generate the output supervised learning assumes that set training data the training set has been consisting set instances that have been properly labeled hand with the correct learning procedure then generates model that attempts meet two sometimes conflicting perform well possible the training and generalize well possible new data this means being simple for some technical definition simple accordance with occam razor discussed unsupervised learning the other assumes training data that has not been and attempts find inherent patterns the data that can then used determine the correct output value for new data combination the two that has recently been explored semi supervised learning which uses combination labeled and unlabeled data typically small set labeled data combined with large amount unlabeled note that cases unsupervised there may training data all speak other the data labeled the training note that sometimes different terms are used describe the corresponding supervised and unsupervised learning procedures for the same type for the unsupervised equivalent classification normally known clustering based the common perception the task involving training data speak and grouping the input data into clusters based some inherent similarity measure the distance between considered vectors dimensional vector space rather than assigning each input instance into one set defined note also that some the terminology for community ecology the term classification used refer what commonly known clustering the piece input data for which output value generated formally termed instance the instance formally described vector features which together constitute description all known characteristics the these feature vectors can seen defining points appropriate multidimensional space and methods for manipulating vectors vector spaces can correspondingly applied such computing the dot product the angle between two typically features are either categorical also known nominal consisting one set unordered such gender male female blood type ordinal consisting one set ordered large medium small integer valued count the number occurrences particular word real valued measurement blood often categorical and ordinal data are grouped likewise for valued and valued furthermore many algorithms work only terms categorical data and require that valued valued data discretized into groups less than between and greater than probabilistic classifiers probabilistic classifier has been suggested that portions this section moved into probabilistic classifier discuss many common pattern recognition algorithms are probabilistic that they use statistical inference find the best label for given unlike other which simply output best label often probabilistic algorithms also output probability the instance being described the given many probabilistic algorithms output list the best labels with associated for some value instead simply single best when the number possible labels fairly small the case classification may set that the probability all possible labels probabilistic algorithms have many advantages over probabilistic they output confidence value associated with their note that some other algorithms may also output confidence but only for probabilistic algorithms this value mathematically grounded probability theory non probabilistic confidence values can general not given any specific and only used compare against other confidence values output the same correspondingly they can abstain when the confidence choosing any particular output too because the probabilities probabilistic recognition algorithms can more effectively incorporated into larger learning way that partially completely avoids the problem error propagation how many feature variables are feature selection algorithms attempt directly prune out redundant irrelevant general introduction feature selection which summarizes approaches and has been the complexity selection because its monotonous optimization problem where given total features the powerset consisting all subsets features need the branch bound algorithm does reduce this complexity but intractable for medium large values the number available features for scale comparison selection algorithms see techniques transform the raw feature vectors feature extraction are sometimes used prior application the matching for feature extraction algorithms attempt reduce dimensionality feature vector into dimensionality vector that easier work with and encodes less using mathematical techniques such principal components analysis the distinction between feature selection and feature extraction that the resulting features after feature extraction has taken place are different sort than the original features and may not easily while the features left after feature selection are simply subset the original problem statement supervised formally the problem supervised pattern recognition can stated given unknown function the ground truth that maps input instances output labels along with training data assumed represent accurate examples the produce function that approximates closely possible the correct mapping for the problem filtering then some representation email and either spam order for this defined approximates closely needs defined decision theory this defined specifying loss function that assigns specific value loss resulting from producing incorrect the goal then minimize the expected loss with the expectation taken over the probability distribution neither the distribution nor the ground truth function are known but can computed only empirically collecting large number samples and labeling them using the correct value consuming which typically the limiting factor the amount data this sort that can the particular loss function depends the type label being for the case classification the simple zero one loss function often this corresponds simply assigning loss any incorrect labeling and implies that the optimal classifier minimizes the error rate independent test data counting the fraction instances that the learned function labels which equivalent maximizing the number correctly classified the goal the learning procedure then minimize the error rate maximize the correctness typical test for probabilistic pattern the problem instead estimate the probability each possible output label given particular input estimate function the form where the feature vector input and the function typically parameterized some parameters discriminative approach the estimated generative approach however the inverse probability instead estimated and combined with the prior probability using bayes rule when the labels are continuously distributed regression analysis the denominator involves integration rather than the value typically learned using maximum posteriori estimation this finds the best value that simultaneously meets two conflicting perform well possible the training data smallest error rate and find the simplest possible essentially this combines maximum likelihood estimation with regularization procedure that favors simpler models over more complex bayesian context the regularization procedure can viewed placing prior probability different values mathematically where the value used for the subsequent evaluation and the posterior probability given the bayesian approach this instead choosing single parameter vector the probability given label for new instance computed integrating over all possible values weighted according the posterior frequentist bayesian approach pattern the first pattern classifier the linear discriminant presented fisher was developed the frequentist tradition the frequentist approach entails that the model parameters are considered but the parameters are then computed from the collected for the linear these parameters are precisely the mean vectors and the covariance matrix also the probability each class estimated from the collected note that the usage bayes rule pattern classifier does not make the classification approach bayesian statistics has its origin greek philosophy where distinction was already made between the priori and the posteriori knowledge later kant defined his distinction between what priori known before observation and the empirical knowledge gained from bayesian pattern the class probabilities can chosen the which are then moreover experience quantified priori parameter values can weighted with empirical observations using the beta conjugate prior and dirichlet distributions the bayesian approach facilitates seamless intermixing between expert knowledge the form subjective and objective probabilistic pattern classifiers can used according frequentist bayesian uses the face was automatically detected within medical pattern recognition the basis for computer aided diagnosis systems cad describes procedure that supports the interpretations and people counter other typical applications pattern recognition techniques are automatic speech recognition classification text into several categories spam spam email the automatic recognition handwritten postal codes postal automatic recognition images human handwriting image extraction from medical the last two examples form the subtopic image analysis pattern recognition that deals with digital images input pattern recognition optical character recognition classic example the application pattern see ocr example the method signing name was captured with stylus and overlay starting citation needed the speed relative relative acceleration and pressure used uniquely identify and confirm banks were first offered this but were content collect from the fdic for any bank fraud and did not want inconvenience citation needed artificial neural networks neural net and deep learning have many world applications image few identification and license plate fingerprint analysis and face medical screening for cervical cancer breast defence various navigation and guidance target recognition shape recognition technology for discussion the aforementioned applications neural networks image see pattern making sense and identifying the objects see closely related which explains how the sensory inputs receive are made pattern recognition can thought two different the first being template matching and the second being feature template pattern used produce items the same the matching hypothesis suggests that incoming stimuli are compared with templates the long term there the stimulus feature detection such the pandemonium system for classifying letters suggest that the stimuli are broken down into their component parts for for capital has three horizontal lines and one vertical algorithms algorithms for pattern recognition depend the type label whether learning supervised and whether the algorithm statistical statistical statistical algorithms can further categorized generative discriminative this article contains embedded lists that may poorly unverified indiscriminate please help clean meet quality where incorporate items into the main body the may classification algorithms supervised algorithms predicting categorical labels statistical classification linear discriminant analysis quadratic discriminant analysis maximum entropy classifier aka logistic regression multinomial logistic regression note that logistic regression algorithm for despite its the name comes from the fact that logistic regression uses extension linear regression model model the probability input being particular nonparametric decision trees decision lists kernel estimation and neighbor algorithms naive bayes classifier neural networks layer perceptrons support vector machines gene expression programming clustering algorithms unsupervised algorithms predicting categorical labels cluster analysis categorical mixture models deep learning methods citation needed hierarchical clustering agglomerative means clustering correlation clustering kernel principal component analysis kernel ensemble learning algorithms supervised meta algorithms for combining multiple learning algorithms ensemble learning boosting bootstrap aggregating bagging ensemble averaging mixture experts hierarchical mixture experts general algorithms for predicting structured sets labels bayesian networks markov random fields multilinear subspace learning algorithms predicting labels multidimensional data using tensor representations unsupervised multilinear principal component analysis real valued sequence labeling algorithms predicting sequences real valued labels sequence labeling supervised kalman filters particle filters regression algorithms predicting real valued labels regression gaussian process regression linear regression and extensions neural networks and deep learning methods independent component analysis principal components analysis sequence labeling algorithms predicting sequences categorical labels supervised conditional random fields hidden markov models maximum entropy markov models recurrent neural networks hidden markov models see also adaptive resonance theory cache language model compound term processing aided diagnosis data mining deep learning list numerical analysis software list numerical libraries machine learning multilinear subspace learning neocognitron perception perceptual learning predictive analytics prior knowledge for pattern recognition sequence mining template matching contextual image classification 