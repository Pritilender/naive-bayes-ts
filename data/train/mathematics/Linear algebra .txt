mathematics
linear algebra navigation search elementary algebra please help improve discuss these issues the talk page this article may confusing unclear readers please help clarify the article suggestions may found the talk page may this article needs additional citations for verification please help improve this article adding citations reliable sources unsourced material may challenged and may euclidean space origin linear algebra the branch mathematics concerning vector spaces and linear mappings between such includes the study planes and but also concerned with properties common all vector the set points with coordinates that satisfy linear equation form hyperplane dimensional the conditions under which set hyperplanes intersect single point important focus study linear such investigation initially motivated system linear equations containing several such equations are naturally represented using the formalism matrices and linear algebra central both pure and applied for abstract algebra arises relaxing the axioms vector leading number functional analysis studies the dimensional version the theory vector combined with linear algebra facilitates the solution linear systems differential equations techniques from linear algebra are also used analytic geometry engineering physics natural sciences computer science computer animation and the social sciences particularly economics because linear algebra such developed nonlinear mathematical models are sometimes approximated linear contents history educational history scope study vector spaces linear transformations subspaces span and basis vectors matrix theory eigenvalues and eigenvectors inner product spaces some main useful theorems applications solution linear systems least squares best fit line fourier series expansion quantum mechanics geometric introduction introduction linear transformations coordinates relative basis inverse image generalizations and related topics see also notes further reading external links online books history the study linear algebra first emerged from the study determinants which were used solve systems linear determinants were used leibniz and gabriel cramer devised cramer rule for solving linear systems later gauss further developed the theory solving linear systems using gaussian elimination which was initially listed advancement geodesy the study matrix algebra first emerged england the hermann grassmann published his theory which included foundational new topics what today called linear james joseph sylvester introduced the term which latin for womb while studying compositions linear arthur cayley was led define matrix multiplication and crucially cayley used single letter denote thus treating matrix aggregate also realized the connection between matrices and and wrote there would many things say about this theory matrices which seems precede the theory seyin tevfik pasha wrote the book titled linear the first modern and more precise definition vector space was introduced peano theory linear transformations dimensional vector spaces had linear algebra first took its modern form the first half the twentieth when many ideas and methods previous centuries were generalized abstract algebra the use matrices quantum mechanics special relativity and statistics helped spread the subject linear algebra beyond pure the development computers led increased research efficient algorithms for gaussian elimination and matrix and linear algebra became essential tool for modelling and the origin many these ideas discussed the articles determinants and gaussian elimination educational history with respect the history mathematics linear algebra first appeared graduate textbooks the and undergraduate textbooks the following work the school mathematics study group high schools asked grade students matrix formerly reserved for the france during the educators attempted instill linear algebra the first year secondary school through curriculum affine dimensional vector this lead backlash the that removed linear algebra from the the the based linear algebra curriculum study group recommended that undergraduate linear algebra courses take based opposed theoretical scope study vector spaces the main structures linear algebra are vector spaces vector space over field set together with two binary operations elements are called vectors and elements are called scalars the first vector addition takes any two vectors and and outputs third vector the second operation takes any scalar and any vector and outputs new vector view the first where the multiplication done rescaling the vector scalar the multiplication called scalar multiplication the operations addition and multiplication vector space satisfy the following axioms the list let and arbitrary vectors and and scalars axiom signification associativity commutativity identity element zero vector inverse elements additive inverse distributivity multiplicative identity elements general vector space may objects any for functions polynomials vectors linear algebra concerned with properties common all vector linear transformations similarly the theory other algebraic linear algebra studies mappings between vector spaces that preserve the space given two vector spaces and over field linear transformation also called linear linear mapping linear map that compatible with addition and scalar for any vectors and scalar additionally for any vectors and scalars when bijective linear mapping exists between two vector spaces that every vector from the second space associated with exactly one the say that the two spaces are isomorphic because isomorphism preserves linear two isomorphic vector spaces are essentially the from the linear algebra point one essential question linear algebra whether mapping isomorphism and this question can answered checking the determinant mapping not linear algebra interested finding its range and the set elements that get mapped called the kernel the linear transformations have geometric for real matrices denote standard planar mappings that preserve the origin subspaces span and basis again analogue with theories other algebraic linear algebra interested subsets vector spaces that are vector spaces these subsets are called linear subspaces for the range and kernel linear mapping are both and are thus often called the range space and the nullspace these are important examples another important way forming subspace take linear combination set vectors where are the set all linear combinations vectors called their span which forms linear combination any system vectors with all zero coefficients the zero vector this the only way express the zero vector linear combination then these vectors are linearly independent given set vectors that span any vector linear combination other vectors and the set not linearly then the span would remain the same remove from the thus set linearly dependent vectors redundant the sense that there will linearly independent subset will span the same therefore are mostly interested linearly independent set vectors that spans vector space which call basis any set vectors that spans contains and any linearly independent set vectors can extended turns out that accept the axiom choice every vector space has nevertheless this basis may and may not even for there exists basis for the real numbers considered vector space over the rationals but explicit basis has been any two bases vector space have the same cardinality which called the dimension the dimension vector space well defined the dimension theorem for vector spaces basis has finite number called dimensional vector dimensional and subspace then dim dim and are subspaces then one often restricts consideration dimensional vector fundamental theorem linear algebra states that all vector spaces the same dimension are giving easy way characterizing vectors matrix theory matrix particular basis allows one construct coordinate system the vector with coordinates the linear combination the condition that span guarantees that each vector can assigned whereas the linear independence assures that these coordinates are unique there only one linear combination the basis vectors that equal this once basis vector space over has been may identified with the coordinate space under this addition and scalar multiplication vectors correspond addition and scalar multiplication their coordinate vectors furthermore and are dimensional and dimensional vector space over and basis and basis have been then any linear transformation may encoded matrix with entries the field called the matrix with respect these two matrices that encode the same linear transformation different bases are called similar matrix theory replaces the study linear which were defined the study which are concrete this major technique distinguishes linear algebra from theories other algebraic which usually cannot parameterized there important distinction between the coordinate space and general dimensional vector space while has standard basis vector space typically does not come equipped with such basis and many different bases exist although they all consist the same number elements equal the dimension one major application the matrix theory calculation determinants central concept linear while determinants could defined free they are usually introduced via specific representation the the value the determinant does not depend the specific turns out that mapping has inverse and only the determinant has inverse every zero real complex number has inverse the determinant then the nullspace determinants have other including systematic way seeing set vectors linearly independent write the vectors the columns and the determinant that matrix the vectors are linearly determinants could also used solve systems linear equations see cramer rule but real gaussian elimination faster eigenvalues and eigenvectors the action linear transformation may quite attention dimensional examples gives indication the variety their one strategy for general dimensional transformation find characteristic that are invariant sets under zero vector such that scalar multiple then the line through and invariant set under and called characteristic vector eigenvector the scalar such that called characteristic value eigenvalue find eigenvector note that where the identity matrix for there nontrivial solutions that det the determinant polynomial and the eigenvalues are not guaranteed exist the field thus often work with algebraically closed field such the complex numbers when dealing with eigenvectors and eigenvalues that eigenvalue will always would particularly nice given transformation taking vector space into itself can find basis for consisting such basis can easily compute the action the transformation any are linearly independent eigenvectors mapping dimensional spaces with not necessarily eigenvalues and then such transformation called diagonalizable matrix since the the transformation represented diagonal matrix because operations like matrix matrix and determinant calculation are simple diagonal computations involving matrices are much simpler can bring the matrix diagonal not all matrices are diagonalizable even over algebraically closed inner product spaces besides these basic linear algebra also studies vector spaces with additional such inner product the inner product example bilinear form and gives the vector space geometric structure allowing for the definition length and formally inner product map that satisfies the following three axioms for all vectors and all scalars conjugate symmetry note that linearity the first positive definiteness can define the length vector and can prove the cauchy schwarz inequality the quantity and can call this quantity the cosine the angle between the two two vectors are orthogonal orthonormal basis basis where all basis vectors have length and are orthogonal each given any dimensional vector orthonormal basis could found the gram schmidt procedure orthonormal bases are particularly nice deal since then the inner product facilitates the construction many useful for given transform can define its hermitian conjugate the linear transform satisfying satisfies call normal turns out that normal matrices are precisely the matrices that have orthonormal system eigenvectors that span some main useful theorems theorems and definitions linear algebra matrix and only the linear map represented the matrix isomorphism any vector space over field dimension isomorphic vector space over corollary any two vector spaces over the same finite dimension are isomorphic each linear map isomorphism and only the determinant applications because the ubiquity vector linear algebra used many fields natural computer and social below are just some examples applications linear solution linear systems system linear equations linear algebra provides the formal setting for the linear combination equations used the gaussian suppose the goal find and describe the the following system linear the elimination algorithm eliminate from all equations below and then eliminate from all equations below this will put the system into triangular form then using each unknown can solved the eliminated from adding then eliminated from adding formally the result now eliminated from adding the result this result system linear equations triangular and the first part the algorithm the last back consists solving for the known reverse can thus seen that can substituted into which can then solved obtain and can substituted into which can solved obtain the system write any system linear equations matrix the solution this system characterized first find particular solution this equation using gaussian then compute the solutions that find the null space the solution set this equation given the number variables equal the number then can characterize when the system has unique since trivial and only det the equation has unique solution and only det least squares best fit line the least squares method used determine the best fit line for set this line will minimize the sum the squares the fourier series expansion fourier series are representation function trigonometric this series expansion extremely useful solving partial differential equations this will not concerned with convergence nice note that all continuous functions have converging fourier series and nice enough discontinuous functions have fourier series that converges the function value most the space all functions that can represented fourier series form vector space technically call functions that have the same fourier series expansion the same function since two different discontinuous functions might have the same fourier moreover this space also inner product space with the inner product the functions sin for and cos for are orthonormal basis for the space expandable can thus use the tools linear algebra find the expansion any function this space terms these basis for find the coefficient take the inner product with and that quantum mechanics quantum mechanics highly inspired notions linear quantum mechanics the physical state particle represented and observables such momentum energy and angular momentum are represented linear operators the underlying vector more the wave function particle describes its physical state and lies the vector space the functions such that and evolves according the schr dinger equation energy represented the operator where the potential energy also known the hamiltonian operator the eigenvalues represents the possible energies that can given particle some state can expand into linear combination eigenstates the component each eigenstate determines the probability measuring the corresponding and the measurement forces the particle assume that eigenstate wave function geometric introduction many the principles and techniques linear algebra can seen the geometry lines real two dimensional plane when formulated using vectors and matrices the geometry points and lines the plane can extended the geometry points and hyperplanes dimensional point coordinates the plane are ordered pairs real and line defined the set points that satisfy the linear equation where the matrix then where the set homogeneous coordinates associated with the point homogeneous coordinates identify the plane with the plane three dimensional the coordinates are obtained from homogeneous coordinates dividing the third component obtain the linear has the important that and are homogeneous coordinates points the then the point also the for any real and now consider two lines and the intersection these two lines defined that satisfy the matrix using homogeneous the point intersection these two lines the unique zero solution these homogeneous the solutions are multiples the following the rows are linearly independent and represent distinct divide through get cramer rule for the solution set two linear equations two notice that this yields point the plane only when the submatrix associated with has zero interesting consider the case three and which yield the matrix which homogeneous form clearly this equation has the solution which not point the plane for solution exist the plane the coefficient matrix must have rank which means its determinant must another way say this that the columns the matrix must linearly introduction linear transformations another way approach linear algebra consider linear functions the two dimensional real plane here denotes the set real let arbitrary vector and consider the linear function given this transformation has the important property that then this shows that the sum vectors map the sum their images this the defining characteristic linear map linear for this where the image space real number the map called linear functional consider the linear functional little more let and the natural basis vectors that now possible see that the columns the matrix are the image the basis vectors this true for any pair vectors used define coordinates suppose select orthogonal unit vector basis and define coordinates vectors this means vector has coordinates such that then have the linear functional where and are the images the basis vectors and this written matrix form coordinates relative basis this leads the question how determine the coordinates vector relative general basis and assume that know the coordinates the and the natural basis and our goal two find the real numbers that that solve this equation for compute the linear coordinate functionals and for the basis which are given the functionals and compute the components along the basis vectors and respectively that which can written matrix form these coordinate functionals have the these equations can assembled into the single matrix thus the matrix formed the coordinate linear functionals the inverse the matrix formed the basis inverse image the set points the plane that map the same image under the linear functional define line this line the image the inverse this inverse image the set the points that solve the notice that linear functional operates known values for compute value while the inverse image seeks the values for that yield specific value order solve the first recognize that only one the two unknowns can select and rearrange the equation solve for and obtain the inverse image the set for convenience the free parameter has been relabeled the vector defines the intersection the line with the known the the vector satisfies the homogeneous notice that solution this homogeneous then also the set points linear functional that map zero define the kernel the linear the line can considered the set points the kernel translated the vector generalizations and related topics since linear algebra successful its methods have been developed and generalized other parts module theory one replaces the field scalars the concepts linear span basis and dimension which called rank module still make nevertheless many theorems from linear algebra become false module for not all modules have basis those that are called free modules the rank free module not necessarily not every linearly independent subset module can extended form and not every subset module that spans the space contains multilinear algebra one considers multivariable linear that mappings that are linear each number different this line inquiry naturally leads the idea the dual space the vector space consisting linear maps where the field multilinear maps can described via tensor products elements addition vector addition and scalar there bilinear vector product the vector space called algebra for associative algebras are algebras with associate vector product like the algebra square the algebra functional analysis mixes the methods linear algebra with those mathematical analysis and studies various function such spaces representation theory studies the actions algebraic objects vector spaces representing these objects interested all the ways that this and does finding subspaces invariant under all transformations the the concept eigenvalues and eigenvectors especially algebraic geometry considers the solutions systems polynomial equations there are several related topics the field computer programming that utilizes much the techniques and theorems linear algebra encompasses and refers see also eigenvectors fundamental matrix computer vision linear regression statistical estimation method list linear algebra topics numerical linear algebra simplex method solution technique for linear programs transformation matrix 