mathematics	statistical inference navigation search statistics statistical inference the process drawing conclusions from data that are subject random for observational errors sampling initial requirements such system procedures for inference and induction are that the system should produce reasonable answers when applied defined situations and that should general enough applied across range inferential statistics are used test hypotheses and make estimations using sample whereas descriptive statistics describe inferential statistics infer predictions about larger population that the sample the outcome statistical inference may answer the question what should done where this might decision about making further experiments about drawing conclusion before implementing some organizational governmental contents introduction scope comparison descriptive statistics models and assumptions degree assumptions importance valid assumptions approximate distributions randomization based models model based analysis randomized experiments modes inference frequentist inference examples frequentist inference frequentist objectivity and decision theory bayesian inference examples bayesian inference bayesian subjectivity and decision theory other modes inference besides frequentist and information and computational complexity fiducial inference structural inference inference topics see also notes references further reading external links introduction scope for the most statistical inference makes propositions about using data drawn from the population interest via some form random more data about random process obtained from its observed behavior during finite period given parameter hypothesis about which one wishes make statistical inference most often statistical model the random process that supposed generate the which known when randomization has been and particular realization the random set the conclusion statistical inference statistical proposition citation needed some common forms statistical proposition estimate particular value that best approximates some parameter confidence interval set interval constructed using dataset drawn from population under repeated sampling such such intervals would contain the true parameter value with the probability the stated confidence level credible interval set values for posterior rejection hypothesis clustering classification data points into groups comparison descriptive statistics statistical inference generally distinguished from descriptive statistics simple descriptive statistics can thought being just straightforward presentation which modeling decisions made data analyst have had minimal models and assumptions statistical model statistical assumptions any statistical inference requires some statistical model set assumptions concerning the generation the observed data and similar descriptions statistical models usually emphasize the role population quantities about which wish draw descriptive statistics are typically used preliminary step before more formal inferences are degree assumptions statisticians distinguish between three levels modeling fully parametric the probability distributions describing the generation process are assumed fully described family probability distributions involving only finite number unknown for one may assume that the distribution population values truly with unknown mean and and that datasets are generated random sampling the family generalized linear models widely used and flexible class parametric non parametric the assumptions made about the process generating the data are much less than parametric statistics and may for every continuous probability distribution has which may estimated using the sample median the hodges sen estimator which has good properties when the data arise from simple random semi parametric this term typically implies assumptions fully and parametric for one may assume that population distribution has finite furthermore one may assume that the mean response level the population depends truly linear manner some covariate parametric but not make any parametric assumption describing the variance around that mean about the presence possible form any heteroscedasticity more semi parametric models can often separated into and random components one component treated parametrically and the other the known cox model set parametric importance valid assumptions whatever level assumption correctly calibrated inference general requires these assumptions that the generating mechanisms really have been correctly incorrect assumptions random sampling can invalidate statistical more complex and fully parametric assumptions are also cause for for incorrectly assuming the cox model can some cases lead faulty incorrect assumptions normality the population also invalidates some forms based the use any parametric model viewed skeptically most experts sampling human most sampling when they deal with confidence intervals limit themselves statements about based very large where the central limit theorem ensures that these will have distributions that are nearly normal distribution would totally unrealistic and catastrophically unwise assumption make were dealing with any kind economic here the central limit theorem states that the distribution the sample mean for very large approximately normally the distribution not heavy approximate distributions statistical distance asymptotic theory approximation theory given the difficulty specifying exact distributions sample many methods have been developed for approximating with finite approximation results measure how close limiting distribution approaches the sample distribution for with independent samples the normal distribution approximates two digits the distribution the sample mean for many population the berry esseen theorem yet for many practical the normal approximation provides good approximation the distribution when there are independent according simulation studies and experience following work the advanced statistics uses approximation theory and functional analysis quantify the error this the metric geometry probability distributions this approach quantifies approximation error for the kullback leibler divergence bregman divergence and the hellinger distance with indefinitely large limiting results like the central limit theorem describe the sample limiting one limiting results are not statements about finite and indeed are irrelevant finite however the asymptotic theory limiting distributions often invoked for work with finite for limiting results are often invoked justify the generalized method moments and the use generalized estimating equations which are popular econometrics and biostatistics the magnitude the difference between the limiting distribution and the true distribution the the can assessed using the heuristic application limiting results finite samples common practice many especially with dimensional models with log concave likelihoods such with parameter exponential families randomization based models randomization random sample random assignment for given dataset that was produced randomization the randomization distribution statistic under the defined evaluating the test statistic for all the plans that could have been generated the randomization frequentist randomization allows inferences based the randomization distribution rather than subjective and this important especially survey sampling and design statistical inference from randomized studies also more straightforward than many other bayesian inference randomization also survey sampling use sampling without replacement ensures the exchangeability the sample with the randomized randomization warrants missing random assumption for covariate information objective randomization allows properly inductive many statisticians prefer based analysis data that was generated defined randomization true that fields science with developed theoretical knowledge and experimental randomized experiments may increase the costs experimentation without improving the quality similarly results from randomized experiments are recommended leading statistical authorities allowing inferences with greater reliability than observational studies the same however good observational study may better than bad randomized the statistical analysis randomized experiment may based the randomization scheme stated the experimental protocol and does not need subjective however any some hypotheses cannot tested using objective statistical which accurately describe randomized experiments random some such randomized studies are uneconomical model based analysis randomized experiments standard practice refer statistical often linear when analyzing data from randomized however the randomization scheme guides the choice statistical not possible choose appropriate model without knowing the randomization seriously misleading results can obtained analyzing data from randomized experiments while ignoring the experimental common mistakes include forgetting the blocking used experiment and confusing repeated measurements the same experimental unit with independent replicates the treatment applied different experimental modes inference different schools statistical inference have become these schools are not mutually and methods which work well under one paradigm often have attractive interpretations under other the two main paradigms use are frequentist and bayesian inference which are both summarized frequentist inference frequentist inference this paradigm calibrates the production propositions clarification needed complicated considering repeated sampling datasets similar the one considering its characteristics under repeated the frequentist properties any statistical inference procedure can although practice this quantification may examples frequentist inference value confidence interval frequentist objectivity and decision theory one interpretation frequentist inference classical that applicable only terms frequency probability that terms repeated sampling from however the approach neyman develops these procedures terms experiment that before undertaking one decides rule for coming conclusion such that the probability being correct controlled suitable such probability need not have frequentist repeated sampling bayesian inference works terms conditional probabilities probabilities conditional the observed compared the marginal but conditioned unknown probabilities used the frequentist the frequentist procedures significance testing and confidence intervals can constructed without regard utility functions however some elements frequentist such statistical decision theory incorporate utility functions citation needed frequentist developments optimal inference such minimum variance unbiased estimators uniformly most powerful testing make use loss functions which play the role utility loss functions need not explicitly stated for statistical theorists prove that statistical procedure has optimality however loss functions are often useful for stating optimality for median unbiased estimators are optimal under absolute value loss that they minimize expected and least squares estimators are optimal under squared error loss that they minimize expected while statisticians using frequentist inference must choose for themselves the parameters and the estimators test statistic the absence obviously explicit utilities and prior distributions has helped frequentist procedures become widely viewed citation needed bayesian inference bayesian inference the bayesian calculus describes degrees belief using the beliefs are integrate and obey probability bayesian inference uses the available posterior beliefs the basis for making statistical there are several different justifications for using the bayesian examples bayesian inference credible intervals for interval estimation bayes factors for model comparison bayesian subjectivity and decision theory many informal bayesian inferences are based intuitively summaries the for the posterior median and highest posterior density and bayes factors can all motivated this while utility function need not stated for this sort these summaries all depend some stated prior and are generally viewed subjective methods prior construction which not require external input have been proposed but not yet fully formally bayesian inference calibrated with reference explicitly stated loss the bayes the one which maximizes expected averaged over the posterior formal bayesian inference therefore automatically provides optimal decisions decision theoretic sense given data and bayesian inference can made for essentially any although not every statistical inference need have bayesian analyses which are not formally bayesian can incoherent feature bayesian procedures which use proper priors those integrable that they are guaranteed coherent some advocates bayesian inference assert that inference must take place this theoretic and that bayesian inference should not conclude with the evaluation and summarization posterior other modes inference besides frequentist and information and computational complexity minimum description length information theory kolmogorov complexity data mining other forms statistical inference have been developed from ideas information theory and the theory kolmogorov complexity for the minimum description length principle selects statistical models that maximally compress the inference proceeds without assuming counterfactual falsifiable generating probability models for the might done frequentist bayesian however data generating does exist then according shannon source coding theorem provides the mdl description the average and minimizing description length descriptive mdl estimation similar maximum likelihood estimation and maximum posteriori estimation using maximum entropy bayesian priors however mdl avoids assuming that the underlying probability model the mdl principle can also applied without assumptions that the data arose from independent the mdl principle has been applied coding theory information theory linear regression and time series analysis particularly for choosing the degrees the polynomials autoregressive moving average models information theoretic statistical inference has been popular data mining which has become common approach for very large observational and heterogeneous datasets made possible the computer revolution and internet the evaluation statistical inferential procedures often uses techniques criteria from computational complexity theory numerical analysis fiducial inference fiducial inference fiducial inference was approach statistical inference based fiducial probability also known fiducial subsequent this approach has been called extremely limited and even however this argument the same that which shows that called confidence distribution not valid probability distribution and since this has not invalidated the application confidence intervals does not necessarily invalidate conclusions drawn from fiducial structural inference developing ideas fisher and pitman from george barnard developed structural pivotal approach using invariant probabilities group families barnard reformulated the arguments behind fiducial inference restricted class models which fiducial procedures would defined and inference topics the topics below are usually included the area statistical inference statistical assumptions statistical decision theory estimation theory statistical hypothesis testing revising opinions statistics design experiments the analysis variance and regression survey sampling summarizing statistical data see also algorithmic inference bayesian inference fiducial inference induction philosophy statistics predictive inference 